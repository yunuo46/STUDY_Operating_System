# 프로세스가 무엇인가요?
프로세스는 실행 중인 프로그램을 의미하며, 운영체제가 관리하는 자원 할당의 기본 단위이자 스케줄링의 기본 단위이다.

## 프로그램과 프로세스, 스레드의 차이에 대해 설명해 주세요.
- 프로세스 : 현재 실행 중인 프로그램으로 프로그램이 실행 파일로써 메모리에 로드되는 경우 프로세스가 되는 것이다. 이를 위해서 CPU, 메모리, 파일 등의 특정 자원이 요구된다.
- 스레드 : CPU 활용의 기본 단위로 동이랗ㄴ 프로세스에 속하는 다른 스레드들과 코드 섹션,ㅡ 데이터 섹션 등의 os 자원을 공유한다. 스레드 생성은 프로세스 생성 보다 컨텍스트 전환이 빨라 비용적인 소모도 적으며 최신 os에서는 프로세스가 여러 개의 실행 스레드를 가지도록 확장하여 한 번에 여러 작업을 수행할 수 있다.

## PCB가 무엇인가요?
Process Control Block

운영체제에서 특정 프로세스에 대한 모든 정보를 저장하는 데이터 구조로 프로세스 상태, 프로그램 카운터, CPU 레지스터 등의 다양한 정보를 저장한다. 즉, 프로세스를 재시작하는 데에 필요한 모든 데이터를 저장하는 저장소 이다.

- 프로세스 상태 : 프로세스의 현재 상태 (새로 생성, 실행 중, 대기 중, 준비, 종료됨)
- CPU Register : 누산기, 인덱스 레지스터 등의 코드 정보
- Program Counter : 다음에 실행될 명령어의 주소


## 그렇다면, 스레드는 PCB를 갖고 있을까요?
스레드는 프로세스의 PCB를 공유할 뿐 PCB를 가지지 않는다. 다만 Thread Contorl Block이라는 작은 단위의 관리 구조를 가진다.

관리되는 정보는 다음과 같다.

- Thread ID
- Thread State
- Program Counter, Register, Stack Pointer

## 리눅스에서, 프로세스와 스레드는 각각 어떻게 생성될까요?
프로세스
- 부모 프로세스에서 fork 호출하면 새로운 자식 프로세스 만듦
- exec()를 통해 실행.

스레드
- clone() 시스템콜.
- 리눅스에서는 스레드를 경량 프로세스로 취급.
- 개발자가 사용하는 함수는 pthread_creat() 함수

## 자식 프로세스가 상태를 알리지 않고 죽거나, 부모 프로세스가 먼저 죽게 되면 어떻게 처리하나요?
**프로세스의 종료가 일어나는 상황**

- 자식이 할 일을 끝내 exit 했을 때 / 부모가 판단하에 자원을 너무 많이 사용하는, 부모가 끝나서 자식도 강제로 끝내야하는 등의 이유로 자식을 강제 종료 시킬 때

**좀비상태**

- 모든 종료된 프로세스가 잠깐 거치는 상태
- 자식이 종료되고, 부모가 wait을 호출하기 전 상태
- 부모가 wait을 호출해야 자식에게 사용된 자원을 운영체제가 거둬감

**고아상태**

- 자식이 종료되지 않았는데도 부모가 종료되었을 때
- system init 프로세스로 편입된다.

## 리눅스에서, 데몬프로세스에 대해 설명해 주세요.
**데몬 프로세스**

- 사용자가 직접 제어하지 않는 백그라운드에서 실행되는 프로세스
    - 실행시킨 터미널을 닫거나 종료해도 실행되고 있음.
- 시스템이 부팅될 때 자동으로 시작되며, 종료될때까지 계속 실행됨.
- 서버에서는 사용자의 요청을 감지하는(http혹은 https 요청 포트를 주시하고 있음) httpd나 nginx와 같은 데몬 프로세스가 백그라운드에서 실행되고 있다.
- 보통 프로세스 이름 끝에 d가 붙는다.
    - ex) httpd, sshd
- 어떻게 데몬 프로세스가 생성될까?
    1. 부모 프로세스로부터 fork 하여 자식 프로세스 생성
    2. 부모 프로세스는 즉시 종료
    3. 자식 프로세스(데몬)는 고아프로세스가 되어 systemd에 편입.

## 리눅스는 프로세스가 일종의 트리를 형성하고 있습니다. 이 트리의 루트 노드에 위치하는 프로세스에 대해 설명해 주세요.
system init 프로세스(최근에는 systemd)
- init 프로세스는 pid1 할당
- 모든 프로세스의 조상
- 고아 프로세스의 부모
- 시스템 부팅 및 관리
- 초기화

# 프로세스 주소공간에 대해 설명해 주세요.
프로세스의 주소공간은 4가지로 구분됩니다. 
첫번째로, Text(Code) Segment로 프로그램 실행에 필요한 코드들이 기계어로 저장되어 있는 공간을 의미합니다. Read-only로 정적인(static) 공간입니다.  

두번째로는 Data Segment입니다. 전역 변수나 Static 변수와 같이 프로그램이 사용할 수 있는 데이터를 저장하는 공간으로 프로그램이 종료되어야 삭제하는 과정을 거칩니다. 

세번째로,  Heap Segment입니다. 동적으로 크기가 변화하는 영역입니다. 동적 할당 데이터를 위한 공간입니다.
(메모리 낮은 주소에서 높은 주소로 할당)

네번째로, Stack Segment입니다. 함수의 호출과 관계있는 지역 변수나 매개변수가 저장되는 영역입니다. 함수의 호출과 함께 할당되고, 함수의 호출이 완료되면 함께 사라집니다. (stack frame) 재귀함수가 너무 깊거나 지역변수가 너무 많다면 stack overflow가 발생
(메모리 높은 주소에서 낮은 주소로 할당)

## 초기화 하지 않은 변수들은 어디에 저장될까요?
초기화 되지 않은 변수는 BSS(Block Started by Symbol) 세그먼트에 저장됩니다.  컴파일 타임에 정적으로 할당된 변수만 적용이 되고, 동적인 변수는 해당하지 않습니다. 데이터 영역 내에 있지만, 별도의 세그먼트로 취급합니다.

## 일반적인 주소공간 그림처럼, Stack과 Heap의 크기는 매우 크다고 할 수 있을까요? 그렇지 않다면, 그 크기는 언제 결정될까요?
매우 크다고 볼 수 없습니다. stack과 Heap 영역 자체는 주어진 적은 크기만 사용이 가능합니다. 하지만 두 영역 사이의 빈 메모리 공간을 활용하여 동적으로 메모리를 할당할 수 있습니다. 스택은 소스 코드를 기반으로 스택의 용량이 결정되어 컴파일시에 stack을 할당하게 됩니다. Heap은 런타임시에 크기가 결정됩니다. 

## Stack과 Heap 공간에 대해, 접근 속도가 더 빠른 공간은 어디일까요?
일반적으로 Stack 접근이 Heap보다 빠르다
Stack

- 지역 변수 접근  → 단순히 **레지스터/스택 포인터 계산**

Heap
- malloc/new → 메모리 관리자가 블록 탐색/관리 → 느림
- 이후 접근 자체는 “메모리 접근” 이지만 배치가 비연속적이다

## 다음과 같이 공간을 분할하는 이유가 있을까요?
1. 안정성
    
    코드영역은 보통 읽기 전용으로 둔다
    
    → 실수로 프로그램이 자기 코드를 덮어쓰지 못하게 해서 버그 & 보안 문제 예방
    
2. 효율적인 메모리 관리
    1. Text : 모든 프로세스가 같은 코드라면 공유 가능(메모리 절약)
    2. Data / BSS : 프로세스별 독립적으로 관리
    3. Heap : 동적 할당이 필요한 만큼 늘어남
    4. Stack : 함수호출 마다 프레임 단위로 관리
    
    → 메모리를 **페이지 단위로 최적화 해서** 배치/보호 할 수 있다
    
3. 확장성
    - Heap/ Stack 형태로 두면, 두 영역이 서로 다른방향으로 자라면서 필요할 때 확장이 가능하다
    - ex) 프로그램 재귀를 깊게 쓰면 스택이 확장되고, 대규모 자료구자가 필요하면 힙이 확장됨

## 스레드의 주소공간은 어떻게 구성되어 있을까요?
스레드는 같은 프로세스와 주소공간(Text, Data, Heap)을 함께 사용한다 하지만, 각자 **자기만의 Stack**을 가진다.

왜 스택만 따로인가?

→ 지역변수, 함수 호출 기록, 리턴 주소는 스레드마다 독립적이어야 충돌하지않기 때문

## "스택"영역과 "힙"영역은 정말 자료구조의 스택/힙과 연관이 있는 걸까요? 만약 그렇다면, 각 주소공간의 동작과정과 연계해서 설명해 주세요.
- 스택은 함수를 관리하기 때문에 자료구조의 스택처럼 LIFO 형태로 동작한다.
- Heap은 이름만 같다.
    - 메모리에서 Heap은 ‘아무렇게 쌓은 더미’를 의미한다고 한다.

## IPC의 Shared Memory 기법은 프로세스 주소공간의 어디에 들어가나요? 그런 이유가 있을까요?
- 어떤 프로세스에 귀속되어 있는 것이 아니라 운영체제에 의해 별도로 생성되어 가상 주소 공간에 배치되어 프로세스 내부에 위치해 있는 것 처럼 사용하는 기법
- stack과 heap 사이에 논리적 주소로 위치해 있다. 같은 메모리 공간에 위치하는 것처럼 보여야 메모리 사용하듯 사용가능하다.

## 스택과 힙영역의 크기는 언제 결정되나요? 프로그램 개발자가 아닌, 사용자가 이 공간의 크기를 수정할 수 있나요?
Stack 영역 최대 크기는 프로세스 시작 시, 혹은 스레드 생성 시 운영체제가 결정합니다.

Heap 영역은 프로그램 실행 중 동적 메모리 요청 시에 점진적으로 확장되며 크기는 시스템 메모리나 OS 정책에 의해 제한됩니다.

사용자는 Stack 크기를 스레드 생성 옵션 등을 통해 변경 가능하고,

Heap 크기는 OS 가상 메모리 설정이나 실행 옵션 등을 통해 간접적으로 조절이 가능합니다.

# 단기, 중기, 장기 스케쥴러에 대해 설명해 주세요.
## 현대 OS에는 단기, 중기, 장기 스케쥴러를 모두 사용하고 있나요?
- 장기 스케쥴러
    - 메모리에 올라갈 프로세스 선택하는 스케쥴러
    - 메모리와 디스크 사이의 스케쥴링 담당(프로세스가 끝날 때 실행되므로 주기가 김)
    - 어떤 프로세스를 Ready Queue로 보낼지 결정
    - 현재 사용 x
        - 가상메모리 사용해서
- 중기 스케쥴러
    - **스와핑**을 담당하는 스케쥴러
        - 메모리가 부족해지면 당장 실행되지 않는 프로세스를 통째로 디스크로 보내고, 필요하면 불러오는
    - 가상메모리 사용해서 page 단위로 스왑되도록 바뀜.
- 단기 스케쥴러
    - CPU 스케쥴러
    - Ready Queue에 있는 프로세스들 중에서 어떤 프로세스에 CPU 할당할지

## 프로세스의 스케쥴링 상태에 대해 설명해 주세요.
New : 프로세스 생성중

Ready : CPU 할당을 기다리는 상태

Running : CPU 점유하여 실행 중

Waiting(Blocked) : I/O 요청 등으로 CPU가 아닌 자원을 기다리는 상태

Terminated : 실행 종료

Suspended : 메모리에서 쫓겨나 보조 기억 장치로 옮겨진 상태(중기 스케줄러 관련)

## preemptive/non-preemptive 에서 존재할 수 없는 상태가 있을까요?
Preemptive 스케줄링 : 실행중인 프로세스가 강제로 CPU를 뺏길 수 있음 → Ready ↔ Running 상태 전환이 자유로움 

Non-preemptive 스케줄링 : 실행 중인 프로세스가 스스로 CPU를 반납 해야함 → Running 상태에서 Ready 상태로의 전환은 불가능

## Memory가 부족할 경우, Process는 어떠한 상태로 변화할까요?
메모리가 부족할 경우, Ready or Waiting 상태에서 Suspeded 상태로 변경된다

→ 운영 체제는 Swap Out을 통해 메모리 공간을 확보

이후 메모리가 확보되면 Swap In 하여 Ready Queue로 복귀